# Slide 1
We can elevate our understanding of our async code's behavior beyond simple print statements.

# Slide 2

**Logs and Traces:**

Logs are invaluable for recording events and errors in our applications, but they might not always offer a complete picture of what's happening within asynchronous code. That's where traces come in. Traces capture the sequence of events across various functions and tasks, providing us with a timeline view of how async operations interact and interleave.

**OpenTelemetry:**

OpenTelemetry is a powerful framework for gathering and analyzing telemetry data, including logs, traces, and metrics. It's designed to be vendor-agnostic and open-source, making it a widely adopted standard in the industry. By integrating OpenTelemetry into our Rust program, we can easily instrument our code to generate detailed traces.

**Jaeger:**

Jaeger is a popular open-source tool for visualizing and analyzing traces.  It can process trace data generated by OpenTelemetry (or other tracing systems) and present it in an intuitive web interface. This allows us to interactively explore the execution flow of our asynchronous code, pinpoint performance bottlenecks, and identify areas for potential optimization.

In the upcoming slides, we'll take a closer look at how to instrument our Rust earthquake retrieval program with OpenTelemetry and visualize the resulting traces using Jaeger. This will give us a deeper understanding of how our asynchronous code operates and help us identify opportunities to improve its performance and reliability.

# Slide 3

We will make our code more organized and easier to work with by refactoring our `main.rs` file. The key change is extracting the core data processing logic from the `main` function into a separate `process` function.

- **`main` Function:**  The `main` function now primarily handles the overall program setup. It's responsible for setting up the OpenTelemetry tracer and any other necessary configurations. It then calls the `process` function to perform the actual data fetching, clustering, and analysis.

- **`process` Function:** This function contains the core logic that was previously inside the `main` function. It takes `end_date` and `back_until` arguments, which determine the time range for the data to be fetched. It performs the following tasks:
    - Fetch earthquake data from the USGS API
    - Combine results into a unified data structure
    - Cluster the earthquake events
    - Calculate statistics for each cluster
    - Print the results

# Slide 4

**Tracing Data Flow:**

This diagram illustrates how we're using OpenTelemetry to trace the flow of data and events within our application. Let's break it down step-by-step:

1. **`tracing` Crate:**
    - At the core, we're using the `tracing` crate, which is a framework for instrumenting Rust code.
    - We insert trace points throughout our code using `tracing` macros like `info!`, `error!`, or custom spans.

2. **`tracing_subscriber` Crate:**
    - The `tracing_subscriber` crate provides a way to subscribe to the trace events emitted by our code.
    - It acts as a central hub, collecting all the trace information.

3. **`fmt` Layer:**
    - This layer within `tracing_subscriber` is responsible for formatting the trace data in a human-readable way.
    - This is often what we see as log messages in our console output.

4. **`EnvFilter` (Optional):**
    - The `EnvFilter` allows us to control which trace events are captured and processed based on environment variables.
    - This is useful for filtering out noisy or irrelevant data.

5. **`tracing_opentelemetry` Layer:**
    - This layer is crucial for OpenTelemetry integration. It takes the trace data from `tracing` and translates it into OpenTelemetry spans.
    - OpenTelemetry spans are standardized units of work that can be exported to various backends.

6. **OTLP Exporters:**
    - OTLP (OpenTelemetry Protocol) exporters are responsible for sending our trace data to a backend system for analysis and storage.
    - In this diagram, we have multiple OTLP exporters, indicating that we can send our traces to different destinations concurrently.

7. **Jaeger:**
    - Jaeger is our chosen backend for visualizing and analyzing the trace data.
    - It receives the traces from the OTLP exporters and presents them in a user-friendly web interface.

**How it Works:**

When our Rust application runs, the `tracing` macros generate trace events. These events flow through the `tracing_subscriber`, where they're formatted and optionally filtered. The `tracing_opentelemetry` layer then transforms the trace events into OpenTelemetry spans, which are sent to the OTLP exporters. Finally, the exporters deliver the traces to Jaeger, where we can visualize and gain insights into our application's behavior.

# Slide 5

First, we need to import a few essential crates (libraries) to use OpenTelemetry effectively:

* **`opentelemetry`:** This crate provides the core building blocks for OpenTelemetry instrumentation. It includes essential data types like `KeyValue`, which we'll use to attach attributes to our traces.

* **`opentelemetry_sdk`:** This crate contains the OpenTelemetry Software Development Kit (SDK). It gives us the tools to create and configure a tracer provider, which is responsible for managing the collection and processing of trace data. The `trace` and `Resource` modules are particularly useful for configuring traces and defining attributes related to our application's resources.

* **`tracing`:**  It enables us to instrument our Rust code with structured logging and spans.  These spans will form the basis for our OpenTelemetry traces.

* **`tracing_subscriber`:** This crate provides the machinery for subscribing to the trace events emitted by our code. It includes layers, filters, and a registry to help us control how the trace data is collected and processed.

# Slide 6

The code snippet you see here configures how log messages will be filtered and displayed in our Rust program.

**`console_env_filter`:**

This is a filter that determines which log messages should be shown in our console. It reads the configuration from the `RUST_LOG` environment variable. This environment variable allows us to control how much detail we want to see in our logs. For example, if we set `RUST_LOG=debug`, we'll see more verbose logging output, including debugging messages. If we set `RUST_LOG=error`, we'll only see error messages. If the `RUST_LOG` variable is not set, the default log level is INFO.

**`console_logger`:**

This component is responsible for formatting the log messages and then sending them to our console. It uses the `console_env_filter` we just defined to ensure that only the log messages that match our desired level of detail are actually printed.

# Slide 7

The OpenTelemetry exporter is a crucial component for sending our trace data to a backend system for analysis and visualization.


* **`otlp_exporter`:** This creates an OpenTelemetry Protocol (OTLP) exporter. OTLP is the standardized protocol that OpenTelemetry uses to transmit telemetry data like traces and metrics. This exporter is how our Rust application will send the collected trace data to a backend where it can be processed and analyzed.
* **`.tonic()`:** This configures the exporter to use the Tonic implementation of gRPC (gRPC is a high-performance, open-source universal RPC framework). gRPC is a common choice for communication between OpenTelemetry components due to its efficiency and reliability.

By default, this exporter is set up to send trace data to `http://localhost:4317`. However, you can easily customize this to point to your own OpenTelemetry collector.

# Slide 8


**`otlp_tracer`:**

- This is the central component of OpenTelemetry in our app. It's responsible for:
    - **Creating Spans:**  Whenever we use `tracing::info!` or decorate a function with the `instrument` attribute, the tracer generates a "span".  Spans record details about specific operations in our code, like the time it took and any relevant attributes.
    - **Managing Context:** It tracks the relationships between spans, creating a tree-like structure that represents the flow of execution in our code.
    - **Sampling:** It can intelligently decide which spans to keep and which to discard based on sampling configurations. This helps manage the volume of trace data.
    - **Exporting Spans:**  It sends the collected spans to the exporter, which we'll discuss next.

**`.with_exporter(otlp_exporter)`:**

- This line connects our tracer pipeline to the OTLP exporter we set up earlier. Now, the tracer knows where to send the trace data it collects.

**`.with_trace_config(...)`:**

- This allows us to customize how traces are handled. We can set the sampling rate (how often traces are recorded) and add resource attributes to our traces.

**`KeyValue::new("service.name", "earthquake_tracing_app")`:**

- This line adds a specific attribute called `service.name` to our spans, giving it the value "earthquake_tracing_app".  This helps us identify which service generated the trace data when we view it in Jaeger later.

**`.install_batch(...)`:**

- We use this to install the tracer provider with a batch processor.  This means the tracer will collect multiple spans together and send them to the exporter in batches, improving efficiency.


# Slide 9

Now, we show the setup for the tracing layer within our OpenTelemetry configuration. This layer is the crucial bridge between the tracing information generated by our Rust code and the OpenTelemetry ecosystem.


**`tracing_env_filter`:**

- This filter is similar to the one we used for logging, but it specifically targets tracing information. It reads configuration details from the `RUST_LOG` environment variable.
- By default, it's set to `LevelFilter::TRACE`, which means all tracing information (spans) will be included unless you explicitly configure it differently in the environment variable.

**`tracing_opentelemetry::layer()`:**

- This line creates the OpenTelemetry layer responsible for converting spans generated by the `tracing` library into OpenTelemetry spans.
- It acts as the translator between the instrumentation we've added to our code using `tracing::info!` or the `#[instrument]` attribute and the OpenTelemetry format.

**`.with_tracer(otlp_tracer)`:**

- This step connects the tracing layer to the OTLP (OpenTelemetry Protocol) tracer we created earlier. This connection is crucial because it tells the layer where to send the converted OpenTelemetry spans for further processing and export.

**`.with_filter(tracing_env_filter)`:**

- Finally, we apply the `tracing_env_filter` to this layer. This allows us to control which spans are actually processed and sent to the tracer based on the configuration in the `RUST_LOG` environment variable. This filtering capability helps us manage the amount of trace data we collect and focus on the most relevant information.

# Slide 10

Now, let's finalize our OpenTelemetry setup by combining all the components we've created and ensuring that the traces are correctly collected and exported.


1. **Combine Layers:**
   - We create a `subscriber` that combines our OpenTelemetry `telemetry` layer and the `console_logger` layer. This means that both tracing data (spans) and log messages will be collected and processed by this combined subscriber.

2. **Set Global Default:**
   - The `set_global_default` function makes this combined subscriber the default for the entire application. This means that any `tracing` events (logs or spans) emitted from anywhere in our code will be handled by this subscriber.

3. **Process Data:**
   - We call our `process` function, which contains the core logic of our earthquake data retrieval and analysis. Since the `process` function and any functions it calls are instrumented with `tracing` macros or the `instrument` attribute, the spans created within them will be picked up by the OpenTelemetry layer and sent to the exporter.

4. **Shutdown Gracefully:**
   - The final steps involve adding a delay before shutting down the tracer provider. This ensures that all pending trace data has been flushed and sent to the collector.
   - We include a log message indicating that the shutdown is in progress.
   - Finally, we call `shutdown_tracer_provider()` to gracefully shut down the tracer provider and release any resources it's holding.

By following these steps, we've completed the integration of OpenTelemetry into our Rust earthquake application. The next time we run our program, it will generate trace data that we can visualize and analyze using a tool like Jaeger.

# Slide 11

To get started, we need to run Jaeger locally:


This command launches an all-in-one Jaeger instance within a Docker container, making it easily accessible on our local machine. The container exposes ports `16686` for the Jaeger UI and `6831` for receiving trace data.

Once Jaeger is running, we navigate to `http://localhost:16686/` in our web browser. This will open the Jaeger UI, where we can:

* **Select Service:** Choose our "earthquake_tracing_app" service from the dropdown menu. Remember, this is the service name we set earlier when configuring the OpenTelemetry tracer.

* **Search Traces:** Click the "Find Traces" button to retrieve the traces generated by our Rust application.

* **Visualize Traces:** Jaeger presents the traces in an intuitive graphical view, allowing us to see the flow of execution, the duration of each span, and any associated metadata. We can explore the trace hierarchy, zoom in on specific spans, and identify potential performance bottlenecks or areas where our code might be spending more time than expected.

* **Analyze Errors:** If our application encountered any errors during the traced execution, Jaeger will highlight those errors in the trace view. This helps us quickly pinpoint the root cause of issues and understand how they affected the overall performance of our code.
